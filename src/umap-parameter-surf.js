import { initDatabase, allAsync } from './database-vec.js';
import chalk from 'chalk';
import ora from 'ora';
import { program } from 'commander';
import fs from 'fs';
import path from 'path';

// For now, let's create a stub that will use Python UMAP since umap-js might need installation
// We'll make it export the data in the perfect format for regl-scatterplot

async function loadEmbeddingsFromDB(maxSamples = 2000) {
  await initDatabase();
  
  const spinner = ora('Loading embeddings from database...').start();
  
  // Get embeddings with metadata
  const embeddings = await allAsync(`
    SELECT 
      c.*,
      n.file_path,
      n.title,
      n.folder_path,
      n.folder_depth
    FROM chunks c
    JOIN notes n ON c.note_id = n.id
    ORDER BY RANDOM()
    LIMIT ?
  `, [maxSamples]);
  
  if (embeddings.length === 0) {
    spinner.fail('No embeddings found');
    return null;
  }
  
  spinner.succeed(`Loaded ${embeddings.length} embeddings`);
  
  // Extract embedding vectors from sqlite-vec
  const spinner2 = ora('Extracting embedding vectors...').start();
  
  const vectors = [];
  const metadata = [];
  
  for (const row of embeddings) {
    try {
      // Get the actual embedding vector from sqlite-vec
      const vectorResult = await allAsync(
        'SELECT embedding FROM vec_embeddings WHERE rowid = ?', 
        [row.id]
      );
      
      if (vectorResult.length > 0) {
        // Convert binary blob to float array
        const buffer = vectorResult[0].embedding;
        const floatArray = new Float32Array(buffer.buffer, buffer.byteOffset, buffer.byteLength / 4);
        vectors.push(Array.from(floatArray));
        
        metadata.push({
          id: row.id,
          chunk_text: row.chunk_text,
          file_path: row.file_path,
          title: row.title,
          folder_path: row.folder_path || 'root',
          folder_depth: row.folder_depth || 0,
          word_count: row.word_count,
          relative_position: row.relative_position,
          has_code: row.has_code,
          has_links: row.has_links,
          has_tags: row.has_tags,
          chunk_index: row.chunk_index
        });
      }
    } catch (error) {
      // Skip this embedding if there's an error
      continue;
    }
  }
  
  spinner2.succeed(`Extracted ${vectors.length} embedding vectors`);
  
  return { vectors, metadata };
}

async function exportForPythonUMAP(maxSamples = 2000, mode = 'expanded') {
  console.log(chalk.blue('üó∫Ô∏è  UMAP Parameter Exploration Setup'));
  
  // Load data
  const data = await loadEmbeddingsFromDB(maxSamples);
  if (!data) return;
  
  const { vectors, metadata } = data;
  
  // Export data for Python UMAP processing
  const timestamp = new Date().toISOString().slice(0, 19);
  const exportPath = `exports/umap_input_${timestamp}`;
  
  // Export embeddings as JSON (Python can load this easily)
  fs.writeFileSync(`${exportPath}_embeddings.json`, JSON.stringify(vectors));
  fs.writeFileSync(`${exportPath}_metadata.json`, JSON.stringify(metadata));
  
  // Create Python script for UMAP parameter exploration
  const pythonScript = `#!/usr/bin/env python3
"""
UMAP Parameter Exploration for Obsidian embeddings
Auto-generated by Node.js script
"""

import json
import numpy as np
import pandas as pd
from pathlib import Path
import time

try:
    import umap
    from sklearn.preprocessing import StandardScaler
except ImportError:
    print("Install required packages: pip install umap-learn scikit-learn")
    exit(1)

def load_data():
    embeddings = np.array(json.load(open('${exportPath}_embeddings.json')))
    metadata = pd.DataFrame(json.load(open('${exportPath}_metadata.json')))
    return embeddings, metadata

def generate_parameter_grid(mode='expanded'):
    """
    Generate parameter combinations with smart interpolations.
    Modes: 'basic' (9), 'expanded' (32-64), 'comprehensive' (100+)
    """
    
    if mode == 'basic':
        # Original 9 combinations
        n_neighbors = [5, 15, 30]
        min_dists = [0.0, 0.1, 0.5]
        metrics = ['cosine']
    
    elif mode == 'expanded':
        # Smart parameter selection for visual exploration (32-64 combinations)
        # Non-linear progression for n_neighbors (more granular at lower values)
        n_neighbors = [3, 5, 8, 12, 15, 20, 30, 40, 50, 75, 100]
        
        # Fine-grained min_dist for smooth transitions
        min_dists = [0.0, 0.05, 0.1, 0.2, 0.3, 0.5, 0.7, 0.9]
        
        # Include key metrics
        metrics = ['cosine', 'euclidean', 'manhattan']
        
        # Create targeted combinations for visual interest
        param_sets = []
        
        # 1. Full cosine sweep (most important) - 22 combinations
        for n in [3, 5, 15, 30, 50, 100]:
            for d in [0.0, 0.1, 0.3, 0.5]:
                param_sets.append({'n_neighbors': n, 'min_dist': d, 'metric': 'cosine'})
        
        # 2. Euclidean at key points - 12 combinations  
        for n in [5, 15, 30, 50]:
            for d in [0.0, 0.1, 0.5]:
                param_sets.append({'n_neighbors': n, 'min_dist': d, 'metric': 'euclidean'})
        
        # 3. Manhattan for contrast - 6 combinations
        for n in [5, 30, 50]:
            for d in [0.1, 0.3]:
                param_sets.append({'n_neighbors': n, 'min_dist': d, 'metric': 'manhattan'})
        
        # 4. Fine transitions around interesting regions - 12 combinations
        # Low n_neighbors with varied min_dist (tight clusters)
        for d in [0.0, 0.05, 0.1, 0.15]:
            param_sets.append({'n_neighbors': 8, 'min_dist': d, 'metric': 'cosine'})
        
        # Medium n_neighbors with smooth transitions
        for d in [0.2, 0.25, 0.3, 0.35]:
            param_sets.append({'n_neighbors': 20, 'min_dist': d, 'metric': 'cosine'})
        
        # High n_neighbors (global structure)
        for d in [0.4, 0.6, 0.8, 1.0]:
            param_sets.append({'n_neighbors': 75, 'min_dist': d, 'metric': 'cosine'})
        
        # Total: ~52 combinations
        return param_sets
    
    elif mode == 'comprehensive':
        # Full grid (100+ combinations) - use sparingly
        param_sets = []
        for n in [3, 5, 8, 12, 15, 20, 25, 30, 40, 50, 75, 100, 150]:
            for d in [0.0, 0.05, 0.1, 0.15, 0.2, 0.3, 0.4, 0.5, 0.7, 0.9, 1.0]:
                for m in ['cosine', 'euclidean']:
                    param_sets.append({'n_neighbors': n, 'min_dist': d, 'metric': m})
        return param_sets
    
    # Default grid construction for basic mode
    param_sets = []
    for n in n_neighbors:
        for d in min_dists:
            for m in metrics:
                param_sets.append({'n_neighbors': n, 'min_dist': d, 'metric': m})
    
    return param_sets

def explore_umap_parameters(mode='expanded', batch_process=True):
    embeddings, metadata = load_data()
    print(f"Loaded {len(embeddings)} embeddings with {len(embeddings[0])} dimensions")
    
    # Get parameter combinations
    param_sets = generate_parameter_grid(mode)
    print(f"\\nExploring {len(param_sets)} parameter combinations in '{mode}' mode")
    
    results = []
    
    # Pre-compute PCA for faster UMAP (optional but recommended)
    if len(embeddings[0]) > 100 and batch_process:
        print("\\nApplying PCA preprocessing for faster computation...")
        from sklearn.decomposition import PCA
        pca = PCA(n_components=50, random_state=42)
        embeddings_pca = pca.fit_transform(embeddings)
        print(f"Reduced from {len(embeddings[0])} to 50 dimensions")
    else:
        embeddings_pca = embeddings
    
    # Folder colors for consistency
    folder_counts = metadata['folder_path'].value_counts()
    top_folders = folder_counts.head(8).index.tolist()
    
    # Batch processing for efficiency
    if batch_process and len(param_sets) > 20:
        print("\\nUsing batch processing for efficiency...")
        # Process in batches to show progress
        batch_size = 10
        for batch_start in range(0, len(param_sets), batch_size):
            batch_end = min(batch_start + batch_size, len(param_sets))
            print(f"\\nProcessing batch {batch_start//batch_size + 1}/{(len(param_sets) + batch_size - 1)//batch_size}")
            
            for i in range(batch_start, batch_end):
                params = param_sets[i]
                process_single_umap(i, params, embeddings_pca, metadata, top_folders, results, param_sets)
    else:
        for i, params in enumerate(param_sets):
            process_single_umap(i, params, embeddings_pca, metadata, top_folders, results, param_sets)

def process_single_umap(i, params, embeddings, metadata, top_folders, results, param_sets):
        print(f"  [{i+1:2d}/{len(param_sets)}] n={params['n_neighbors']:3d}, d={params['min_dist']:.2f}, {params['metric']:9s}", end='')
        
        start_time = time.time()
        
        try:
            reducer = umap.UMAP(
                n_components=2,
                random_state=42,
                **params
            )
            
            embedding_2d = reducer.fit_transform(embeddings)
            compute_time = time.time() - start_time
            
            # Prepare data for regl-scatterplot
            scatterplot_data = []
            for idx, (x, y) in enumerate(embedding_2d):
                meta = metadata.iloc[idx]
                folder_idx = top_folders.index(meta['folder_path']) if meta['folder_path'] in top_folders else 8
                
                scatterplot_data.append({
                    'x': float(x),
                    'y': float(y),
                    'folder': meta['folder_path'],
                    'folderColor': folder_idx,
                    'wordCount': int(meta['word_count']) if meta['word_count'] else 0,
                    'relativePosition': float(meta['relative_position']) if meta['relative_position'] else 0,
                    'hasCode': 1 if meta['has_code'] else 0,
                    'hasLinks': 1 if meta['has_links'] else 0,
                    'hasTags': 1 if meta['has_tags'] else 0,
                    'chunkIndex': int(meta['chunk_index']) if meta['chunk_index'] else 0,
                    'folderDepth': int(meta['folder_depth']) if meta['folder_depth'] else 0,
                    'title': str(meta['title']) if meta['title'] else '',
                    'chunkText': str(meta['chunk_text'])[:100] + '...' if meta['chunk_text'] else '',
                    'filePath': str(meta['file_path']) if meta['file_path'] else ''
                })
            
            results.append({
                'parameters': params,
                'data': scatterplot_data,
                'metadata': {
                    'computeTime': compute_time,
                    'sampleCount': len(embeddings),
                    'topFolders': top_folders,
                    'parameterString': f"N{params['n_neighbors']}_D{params['min_dist']:.1f}_{params['metric'][:3]}"
                }
            })
            
            print(f" ‚úì {compute_time:.1f}s")
            
        except Exception as e:
            print(f" ‚úó Failed: {e}")
            return
    
    # Export results with enhanced metadata
    output = {
        'timestamp': '${timestamp}',
        'mode': mode,
        'totalSamples': len(embeddings),
        'parameterCount': len(param_sets),
        'results': results,
        'parameterSpace': {
            'n_neighbors': sorted(list(set(p['n_neighbors'] for p in param_sets))),
            'min_dist': sorted(list(set(p['min_dist'] for p in param_sets))),
            'metrics': sorted(list(set(p['metric'] for p in param_sets)))
        },
        'colorSchemes': {
            'folder': {
                'name': 'Folder',
                'property': 'folderColor',
                'type': 'categorical',
                'labels': top_folders + ['Other']
            },
            'wordCount': {
                'name': 'Word Count',
                'property': 'wordCount',
                'type': 'continuous'
            },
            'position': {
                'name': 'Position in Note',
                'property': 'relativePosition',
                'type': 'continuous'
            },
            'contentType': {
                'name': 'Content Type',
                'properties': ['hasCode', 'hasLinks', 'hasTags'],
                'type': 'categorical'
            }
        }
    }
    
    output_file = f'exports/umap_parameter_surf_${timestamp}.json'
    with open(output_file, 'w') as f:
        json.dump(output, f, indent=2)
    
    print(f"\\n‚úÖ UMAP Parameter Exploration Complete!")
    print(f"üìÅ Results exported to: {output_file}")
    print(f"üé® Ready for regl-scatterplot in Vue.js!")
    
    # Print summary
    print("\\nüìä Results Summary:")
    for i, result in enumerate(results):
        p = result['parameters']
        t = result['metadata']['computeTime']
        print(f"{i+1:2d}. N={p['n_neighbors']:2d}, D={p['min_dist']:.1f}, {p['metric'][:8]:8s} - {t:.1f}s")

if __name__ == "__main__":
    import sys
    mode = sys.argv[1] if len(sys.argv) > 1 else 'expanded'
    explore_umap_parameters(mode=mode)
`;

  fs.writeFileSync(`${exportPath}_umap_explorer.py`, pythonScript);
  
  console.log(chalk.green('\\n‚úÖ UMAP exploration data exported!'));
  console.log(chalk.cyan(`üìÅ Files created:`));
  console.log(`   ‚Ä¢ ${exportPath}_embeddings.json`);
  console.log(`   ‚Ä¢ ${exportPath}_metadata.json`);
  console.log(`   ‚Ä¢ ${exportPath}_umap_explorer.py`);
  
  console.log(chalk.yellow('\\nüêç To run UMAP parameter exploration:'));
  console.log(`cd exports && python ${path.basename(exportPath)}_umap_explorer.py`);
  
  console.log(chalk.blue('\\nüí° This will generate a JSON file optimized for regl-scatterplot!'));
  
  return exportPath;
}

// CLI
program
  .name('umap-parameter-surf')
  .description('Generate UMAP parameter exploration data for regl-scatterplot')
  .option('-s, --samples <number>', 'maximum samples to use', '2000')
  .option('-m, --mode <mode>', 'parameter grid mode: basic, expanded, comprehensive', 'expanded')
  .action(async (options) => {
    const maxSamples = parseInt(options.samples);
    const mode = options.mode;
    await exportForPythonUMAP(maxSamples, mode);
  });

program.parse();